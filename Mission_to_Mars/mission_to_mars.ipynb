{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from splinter import Browser\n",
    "import selenium\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NASA Mars News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take the url, get a request out of it, then make some soup\n",
    "url = \"https://mars.nasa.gov/news/?page=0&per_page=40&order=publish_date+desc%2Ccreated_at+desc&search=&category=19%2C165%2C184%2C204%3A19&blank_scope=Latest\"\n",
    "request = requests.get(url)\n",
    "soup = BeautifulSoup(request.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next to find all the image containers and assign them as a list to a variable\n",
    "container_box = soup.find_all('div', class_='image_and_description_container')\n",
    "\n",
    "#From inside those containers I can get the Title and Paragraph of the first story and assing to variables \n",
    "title = container_box[0].find_all('img')[1]['alt']\n",
    "para = container_box[0].find_all('a')[0].text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JPL Mars Space Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an executable path, initialize chrome driver\n",
    "executable_path = {\"executable_path\": \"chromedriver.exe\"}\n",
    "browser = Browser(\"chrome\", **executable_path)\n",
    "\n",
    "#Next to input the website and have the driver visit the website\n",
    "jpl_url = \"https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars\"\n",
    "browser.visit(jpl_url)\n",
    "\n",
    "#From here we need to use splinter functionality and click a couple of buttons to get to the right page\n",
    "browser.click_link_by_id('full_image')\n",
    "moreButton = browser.links.find_by_partial_text('more info')\n",
    "moreButton.click()\n",
    "\n",
    "#Now that we are at the right page, we copy this url and make soup from it\n",
    "featured_url = browser.html\n",
    "featured_image = BeautifulSoup(featured_url, \"html.parser\")\n",
    "\n",
    "#Now to navigate the soup and find thte image then same it to a variable\n",
    "jpl_image_url = featured_image.select_one(\"figure.lede a img\").get(\"src\")\n",
    "full_feature_url = f\"https://www.jpl.nasa.gov/{jpl_image_url}\"\n",
    "\n",
    "#Finally to closer the browser to be proper\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mars Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an executable path, initialize chrome driver\n",
    "executable_path = {\"executable_path\": \"chromedriver.exe\"}\n",
    "browser = Browser(\"chrome\", **executable_path)\n",
    "\n",
    "#Next to input the website and have the driver visit the website\n",
    "mars_twit_url = \"https://twitter.com/marswxreport?lang=en\"\n",
    "browser.visit(mars_twit_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Now that we are at the right page, we copy this url and make soup from it\n",
    "mar_twit_url = browser.html\n",
    "mar_twit = BeautifulSoup(mar_twit_url,\"html.parser\")\n",
    "\n",
    "#Next to find the container of tweets and assign to a variable\n",
    "twits = mar_twit.find_all(\"div\", class_=\"css-901oao r-hkyrab r-1qd0xha r-a023e6 r-16dba41 r-ad9z0x r-bcqeeo r-bnwqim r-qvutc0\")\n",
    "\n",
    "#Next I need to make a conditional loop to make sure the tweet is about the weather and not something else\n",
    "condition = False\n",
    "while condition == False:\n",
    "    i = 0\n",
    "    mars_weather = twits[i].span.text\n",
    "    if (\"InSight sol\"):\n",
    "        condition = True\n",
    "    else:\n",
    "        i = i+1\n",
    "        \n",
    "#Print just to double check\n",
    "print(mars_weather)\n",
    "\n",
    "#Close the browser\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mars Facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First to place the url into a variable\n",
    "mar_fact_url = \"https://space-facts.com/mars/\"\n",
    "\n",
    "#Pandas makes it really easy to grab tables from websites\n",
    "mar_fact = pd.read_html(mar_fact_url)[0]\n",
    "\n",
    "#Now just to give the columns names and set the index\n",
    "mar_fact.columns=[\"Description\", \"Value\"]\n",
    "mar_fact.set_index(\"Description\", inplace=True)\n",
    "\n",
    "#mar_fact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mars Hemispheres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take the url, get a request out of it, then make some soup\n",
    "mar_hemi_url = \"https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars\"\n",
    "request_hemi = requests.get(mar_hemi_url)\n",
    "hemi_soup = BeautifulSoup(request_hemi.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now I need to make variables of the two containers I want\n",
    "hemi_names = hemi_soup.find_all('h3')\n",
    "hemi_links = hemi_soup.find_all('a',class_='itemLink product-item')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we need and empty list to store the dictionaries\n",
    "hemisphere_image_urls = []\n",
    "\n",
    "#A variable for the base url\n",
    "mini_url = \"https://astrogeology.usgs.gov\"\n",
    "\n",
    "#Here I am making a for loop that goes through both my links and gets the information I need from it. \n",
    "for name,links in zip(hemi_names, hemi_links):\n",
    "    hemi_name = name.text.replace(\" Enhanced\",\"\")\n",
    "    full_url = mini_url + links['href']\n",
    "    full_image_request = requests.get(full_url)\n",
    "    full_image_soup = BeautifulSoup(full_image_request.text, 'html.parser')\n",
    "    full_image_link = full_image_soup.find('img', class_=\"wide-image\")\n",
    "    full_image_total_url = mini_url + full_image_link['src']\n",
    "    hemisphere_image_urls.append({'title':hemi_name,\"img_url\":full_image_total_url})\n",
    "    time.sleep(1)\n",
    "    \n",
    "#hemisphere_image_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
